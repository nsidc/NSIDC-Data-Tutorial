{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover, Customize and Access NSIDC DAAC Data\n",
    "\n",
    "This notebook is based off of the [NSIDC-Data-Access-Notebook](https://github.com/nsidc/NSIDC-Data-Access-Notebook) provided through NSIDC's Github organization. \n",
    "\n",
    "Now that we've visualized our study areas, we will first explore data coverage, size, and customization (subsetting, reformatting, reprojection) service availability, and then access those associated files. The __Data access for all datasets__ notebook provides the steps needed to subset and download all the data we'll be using in the final __Visualize and Analyze Data__.\n",
    "\n",
    "___A note on data access options:___\n",
    "We will be pursuing data discovery and access \"programmatically\" using Application Programming Interfaces, or APIs. \n",
    "\n",
    "*What is an API?* You can think of an API as a middle man between an application or end-use (in this case, us) and a data provider. In this case the data provider is both the Common Metadata Repository (CMR) housing data information, and NSIDC as the data distributor. These APIs are generally structured as a URL with a base plus individual key-value-pairs separated by ‘&’.\n",
    "\n",
    "There are other discovery and access methods available from NSIDC including access from the data set landing page 'Download Data' tab (e.g. [ATL07 Data Access](https://nsidc.org/data/atl07?qt-data_set_tabs=1#qt-data_set_tabs)) and [NASA Earthdata Search](https://search.earthdata.nasa.gov/). Programmatic API access is beneficial for those of you who want to incorporate data access into your visualization and analysis workflow. This method is also reproducible and documented to ensure data provenance. \n",
    "\n",
    "Here are the steps you will learn in this customize and access notebook:\n",
    "   \n",
    "1. Search for data programmatically using the Common Metadata Repository API by time and area of interest.\n",
    "2. Determine subsetting, reformatting, and reprojection capabilities for our data of interest.\n",
    "3. Access and customize data using NSIDC's data access and service API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import json\n",
    "import random\n",
    "\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "from pprint import pprint\n",
    "import getpass\n",
    "import requests\n",
    "\n",
    "# This is our functions module. We created several functions used in this notebook and the Visualize and Analyze notebook.\n",
    "import tutorial_helper_functions as fn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure a CMR GraphQL client\n",
    "\n",
    "Using `qgl` we can communicate with the CMR GraphQL endpoint in a standards-based way, allowing for schema introspection. `gql` isn't the only python GraphQL client library out there. Other libraries might provide features you value, like `gql-next`'s static type generation functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* variables: \n",
      "* datasetId: True if any of its associated services support spatial subsetting.\n",
      "* abstract: A brief description of the collection or service the metadata represents.\n",
      "* hasTemporalSubsetting: True if any of its associated services support temporal subsetting.\n",
      "* hasFormats: True if there are multiple supported formats for any services associated with the collection.\n"
     ]
    }
   ],
   "source": [
    "CMR_GRAPHQL_URL = 'https://graphql.earthdata.nasa.gov/api'\n",
    "sample_transport=RequestsHTTPTransport(\n",
    "    url=CMR_GRAPHQL_URL,\n",
    "    retries=3,            # Automatically retry, don't put it on the user!\n",
    ")\n",
    "\n",
    "client = Client(\n",
    "    transport=sample_transport,\n",
    "    fetch_schema_from_transport=True,  # Get the schema as part of the client object\n",
    ")\n",
    "\n",
    "collection_schema = client.schema.get_type('Collection')\n",
    "\n",
    "# Show info about 5 random fields\n",
    "sample_fields = random.sample(list(collection_schema.fields.items()), 5)\n",
    "for fieldname, field in sample_fields:\n",
    "    print(f'* {fieldname}: {field.description}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data availability using the Common Metadata Repository \n",
    "\n",
    "The Common Metadata Repository (CMR) is a high-performance, high-quality, continuously evolving metadata system that catalogs Earth Science data and associated service metadata records. These metadata records are registered, modified, discovered, and accessed through programmatic interfaces leveraging standard protocols and APIs. Note that not all NSIDC data can be searched at the file level using CMR, particularly those outside of the NASA DAAC program. \n",
    "\n",
    "General CMR API documentation: https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html\n",
    "\n",
    "GraphQL endpoint documentation and interactive playground: https://graphql.earthdata.nasa.gov/api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data set and determine version number\n",
    "\n",
    "Data sets are selected by data set IDs (e.g. ATL07). In the CMR API documentation, a data set ids is referred to as a \"short name\". These short names are located at the top of each NSIDC data set landing page in gray above the full title. We are using the Python Requests package to access the CMR. Data are then converted to [JSON](https://en.wikipedia.org/wiki/JSON) format; a language independant human-readable open-standard file format. More than one version can exist for a given data set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'collections': {'items': [{'conceptId': 'C1219248592-LANCEMODIS',\n",
      "                            'datasetId': 'MODIS/Terra Near Real Time (NRT) Sea '\n",
      "                                         'Ice Extent 5-Min L2 Swath 1km',\n",
      "                            'shortName': 'MOD29',\n",
      "                            'versionId': '6NRT'},\n",
      "                           {'conceptId': 'C61468238-NSIDC_ECS',\n",
      "                            'datasetId': 'MODIS/Terra Sea Ice Extent 5-Min L2 '\n",
      "                                         'Swath 1km V005',\n",
      "                            'shortName': 'MOD29',\n",
      "                            'versionId': '5'},\n",
      "                           {'conceptId': 'C1000001160-NSIDC_ECS',\n",
      "                            'datasetId': 'MODIS/Terra Sea Ice Extent 5-Min L2 '\n",
      "                                         'Swath 1km V006',\n",
      "                            'shortName': 'MOD29',\n",
      "                            'versionId': '6'}]}}\n"
     ]
    }
   ],
   "source": [
    "query = gql('''\n",
    "query { \n",
    "  collections(shortName: \"MOD29\") {\n",
    "    items { \n",
    "      shortName\n",
    "      datasetId\n",
    "      conceptId\n",
    "      versionId\n",
    "    }\n",
    "  }\n",
    "}\n",
    "''')\n",
    "\n",
    "# As a user, I don't need to know about \"feed\" or \"entry\", just the fields I'm interested in!\n",
    "response = client.execute(query)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will specify the most recent version for our remaining data set queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select time and area of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a simple dictionary with our short name, version, time, and area of interest. We'll continue to add to this dictionary as we discover more information about our data set. The bounding box coordinates cover our region of interest over the East Siberian Sea and the temporal range covers March 23, 2019. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding Box spatial parameter in decimal degree 'W,S,E,N' format.\n",
    "bounding_box = '140,72,153,80'\n",
    "# Each date in yyyy-MM-ddTHH:mm:ssZ format; date range in start,end format\n",
    "temporal = '2019-03-23T00:00:00Z,2019-03-23T23:59:59Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start our data dictionary with our data set, version, and area and time of interest. \n",
    "\n",
    "**Note that some version IDs include 3 digits and others include only 1 digit. Make sure to enter this value exactly as reported above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider rename to \"dataset_info\"? egi_query? To me, \"data_dict\" indicates there's data \n",
    "# inside. I want the name of the variable to tell me its purpose.\n",
    "data_dict = {'short_name': 'MOD29', \n",
    "             'version': '6',\n",
    "             'bounding_box': bounding_box, \n",
    "             'temporal': temporal }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine how many files exist over this time and area of interest, as well as the average size and total volume of those granules\n",
    "\n",
    "We will use the `gql` library once more, this time to query the CMR granule API. We will look at the results and print the number of granules, average size, and total volume of those granules.\n",
    "\n",
    "Finally, we update the data_dict with the granule count. TODO: Is this necessary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 granules\n",
      "Average size: 2.75\n",
      "Total size: 35.70\n"
     ]
    }
   ],
   "source": [
    "# TODO: Get rid of data_dict entirely? or re-use it here? Currently hardcoding bbox, temporal.\n",
    "# NOTE: GraphQL endpoint currently supports selecting granules by conceptId, not short_name, versionId.\n",
    "query = gql('''\n",
    "query {\n",
    "  granules(conceptId: \"C1000001160-NSIDC_ECS\"\n",
    "           boundingBox: \"140,72,153,80\"\n",
    "           temporal: \"2019-03-23T00:00:00Z,2019-03-23T23:59:59Z\"\n",
    "           limit: 100) {\n",
    "    count\n",
    "    items { granuleSize }\n",
    "           \n",
    "  }\n",
    "}\n",
    "''')\n",
    "response = client.execute(query)\n",
    "\n",
    "# Now that it's a so easy to get the info we need, I think granule_info can go away.\n",
    "granule_sizes = [float(i['granuleSize']) for i in response['granules']['items']][:]\n",
    "\n",
    "print(f\"Found {response['granules']['count']} granules\")\n",
    "print(f\"Average size: {mean(granule_sizes):.2f}\")\n",
    "print(f\"Total size: {sum(granule_sizes):.2f}\")\n",
    "\n",
    "data_dict['gran_num'] = int(response['granules']['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that subsetting, reformatting, or reprojecting can alter the size of the granules if those services are applied to your request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***On your own***: Discover data availability for ATL07\n",
    "\n",
    "Go back to the \"Select data set and determine version number\" heading. Replace all `MOD29` instances with `ATL07` along with its most recent version number, keeping your time and area of interest the same. ***Note that ATL07 has a 3-digit version number.*** How does the data volume compare to MOD29? \n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the subsetting, reformatting, and reprojection services enabled for your data set of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NSIDC DAAC supports customization (subsetting, reformatting, reprojection) services on many of our NASA Earthdata mission collections. Let's discover whether or not our data set has these services available using the `print_service_options` function. If services are available, we will also determine the specific service options supported for this data set, which we will then add to our data dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Earthdata Login credentials\n",
    "\n",
    "An Earthdata Login account is required to query data services and to access data from the NSIDC DAAC. If you do not already have an Earthdata Login account, visit http://urs.earthdata.nasa.gov to register. We will input our credentials below, and we'll add our email address to our dictionary for use in our final access request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = '' # Enter Earthdata Login user name\n",
    "\n",
    "pswd = getpass.getpass('Earthdata Login password: ') # Input and store Earthdata Login password\n",
    "\n",
    "email = '' # Enter email associated with Earthata Login account\n",
    "\n",
    "data_dict['email'] = email # Add to data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create an HTTP session in order to store cookies and pass our credentials to the data service URLs. The capability URL below is what we will query to determine service information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query service capability URL \n",
    "capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{data_dict[\"short_name\"]}.{data_dict[\"version\"]}.xml' \n",
    "\n",
    "# Create session to store cookie and pass credentials to capabilities url\n",
    "session = requests.session() \n",
    "s = session.get(capability_url)\n",
    "response = session.get(s.url,auth=(uid,pswd))\n",
    "response.raise_for_status() # Raise bad request to check that Earthdata Login credentials were accepted "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function provides a list of all available services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.print_service_options(data_dict, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate data dictionary with services of interest\n",
    "\n",
    "We already added our CMR search keywords to our data dictionary, so now we need to add the service options we want to request. A list of all available service keywords for use with NSIDC's access and service API are available in our [Key-Value-Pair table](https://nsidc.org/support/tool/table-key-value-pair-kvp-operands-subsetting-reformatting-and-reprojection-services), as a part of our [Programmatic access guide](https://nsidc.org/support/how/how-do-i-programmatically-request-data-services). For our ATL07 request, we are interested in bounding box, temporal, and variable subsetting. These options crop the data values to the specified ranges and variables of interest. We will enter those values into our data dictionary below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bounding box subsetting:__ Output files are cropped to the specified bounding box extent.\n",
    "\n",
    "__Temporal subsetting:__ Output files are cropped to the specified temporal range extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['bbox'] = '140,72,153,80' # Just like with the CMR bounding box search parameter, this value is provided in decimal degree 'W,S,E,N' format. \n",
    "data_dict['time'] = '2019-03-23T00:00:00,2019-03-23T23:59:59' # Each date in yyyy-MM-ddTHH:mm:ss format; Date range in start,end format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variable subsetting:__ Subsets the data set variable or group of variables. For hierarchical data, all lower level variables are returned if a variable group or subgroup is specified. \n",
    "\n",
    "For ATL07, we will use only strong beams since these groups contain higher coverage and resolution due to higher surface returns. According to the user guide, the spacecraft was in the backwards orientation during our day of interest, setting the `gt*l` beams as the strong beams. \n",
    "\n",
    "We'll use these primary geolocation, height and quality variables of interest for each of the three strong beams. The following descriptions are provided in the [ATL07 Data Dictionary](https://nsidc.org/sites/nsidc.org/files/technical-references/ATL07-data-dictionary-v001.pdf), with additional information on the algorithm and variable descriptions in the [ATBD (Algorithm Theoretical Basis Document)](https://icesat-2.gsfc.nasa.gov/sites/default/files/page_files/ICESat2_ATL07_ATL10_ATBD_r002.pdf).\n",
    "\n",
    "`delta_time`: Number of GPS seconds since the ATLAS SDP epoch. \n",
    "\n",
    "`latitude`: Latitude, WGS84, North=+, Lat of segment center\n",
    "\n",
    "`longitude`: Longitude, WGS84, East=+,Lon of segment center\n",
    "\n",
    "`height_segment_height`: Mean height from along-track segment fit determined by the sea ice algorithm\n",
    "\n",
    "`height_segment_confidence`: Confidence level in the surface height estimate based on the number of photons; the background noise rate; and the error\n",
    "analysis\n",
    "\n",
    "`height_segment_quality`: Height segment quality flag, 1 is good quality, 0 is bad\n",
    "\n",
    "`height_segment_surface_error_est`: Error estimate of the surface height (reported in meters)\n",
    "\n",
    "`height_segment_length_seg`: along-track length of segment containing n_photons_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['coverage'] = '/gt1l/sea_ice_segments/delta_time,\\\n",
    "/gt1l/sea_ice_segments/latitude,\\\n",
    "/gt1l/sea_ice_segments/longitude,\\\n",
    "/gt1l/sea_ice_segments/heights/height_segment_confidence,\\\n",
    "/gt1l/sea_ice_segments/heights/height_segment_height,\\\n",
    "/gt1l/sea_ice_segments/heights/height_segment_quality,\\\n",
    "/gt1l/sea_ice_segments/heights/height_segment_surface_error_est,\\\n",
    "/gt1l/sea_ice_segments/heights/height_segment_length_seg,\\\n",
    "/gt2l/sea_ice_segments/delta_time,\\\n",
    "/gt2l/sea_ice_segments/latitude,\\\n",
    "/gt2l/sea_ice_segments/longitude,\\\n",
    "/gt2l/sea_ice_segments/heights/height_segment_confidence,\\\n",
    "/gt2l/sea_ice_segments/heights/height_segment_height,\\\n",
    "/gt2l/sea_ice_segments/heights/height_segment_quality,\\\n",
    "/gt2l/sea_ice_segments/heights/height_segment_surface_error_est,\\\n",
    "/gt2l/sea_ice_segments/heights/height_segment_length_seg,\\\n",
    "/gt3l/sea_ice_segments/delta_time,\\\n",
    "/gt3l/sea_ice_segments/latitude,\\\n",
    "/gt3l/sea_ice_segments/longitude,\\\n",
    "/gt3l/sea_ice_segments/heights/height_segment_confidence,\\\n",
    "/gt3l/sea_ice_segments/heights/height_segment_height,\\\n",
    "/gt3l/sea_ice_segments/heights/height_segment_quality,\\\n",
    "/gt3l/sea_ice_segments/heights/height_segment_surface_error_est,\\\n",
    "/gt3l/sea_ice_segments/heights/height_segment_length_seg'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data access configurations\n",
    "\n",
    "The data request can be accessed asynchronously or synchronously. The asynchronous option will allow concurrent requests to be queued and processed as orders. Those requested orders will be delivered to the specified email address, or they can be accessed programmatically as shown below. Synchronous requests will automatically download the data as soon as processing is complete. For this tutorial, we will be selecting the asynchronous method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request' # Set NSIDC data access base URL\n",
    "data_dict['request_mode'] = 'async' # Set the request mode to asynchronous\n",
    "data_dict['page_size'] = 2000 # Set the page size to the maximum of 2000, which equals the number of output files that can be returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the data request API endpoint \n",
    "Programmatic API requests are formatted as HTTPS URLs that contain key-value-pairs specifying the service operations that we specified above. We will first create a string of key-value-pairs from our data dictionary and we'll feed those into our API endpoint. This API endpoint can be executed via command line, a web browser, or in Python below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new param_dict with CMR configuration parameters removed from our data_dict \n",
    "param_dict = dict((i, data_dict[i]) for i in data_dict if i!='gran_num' and i!='page_num')\n",
    "\n",
    "param_string = '&'.join(\"{!s}={!r}\".format(k,v) for (k,v) in param_dict.items()) # Convert param_dict to string\n",
    "param_string = param_string.replace(\"'\",\"\") # Remove quotes\n",
    "\n",
    "API_request = f'{base_url}?{param_string}' \n",
    "print(API_request) # Print API base URL + request parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request data and clean up Output folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now download data using the `request_data` function, which utilizes the Python requests library. Our param_dict and HTTP session will be passed to the function to allow Earthdata Login access. The data will be downloaded directly to this notebook directory in a new Outputs folder. The progress of the order will be reported. The data are returned in separate files, so we'll use the `clean_folder` function to remove those individual folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.request_data(param_dict,session)\n",
    "fn.clean_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review, we have explored data availability and volume over a region and time of interest, discovered and selected data customization options, constructed API endpoints for our requests, and downloaded data. Let's move on to the analysis portion of the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
