{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src='./img/nsidc_logo.png'/>\n",
    "\n",
    "# **IceFlow**\n",
    "### Point Cloud Data Access\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "## Visualizing large Data Sets\n",
    "IceFlow and ICESat-2 data sets are by nature big data sets that require some special considerations when working with it. The main constraint if you don't have a super computer is memory. The average granule size is in the 10s of Megabyte for ICESat-2 and could be Gigabytes in IceFlow depending on the order/subsetting. \n",
    "\n",
    "This is when libraries like dask, vaex and others come into play. This notebook will show you how to use some basic plotting techniques using matplotlib and geopandas + vaex to work effectively with lidar data from IceFlow and ICESat-2 data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import glob\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import vaex\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "from iceflow.processing import IceFlowProcessing as ifp\n",
    "\n",
    "# filepath = 'data/atm1b_data_2020-07-10T15-32.hdf5'\n",
    "# df_k = ifp.get_common_dictionary('ATM')\n",
    "\n",
    "filepath = 'data/twaties-test-GLAH06-2000-2010.h5'\n",
    "df_key = ifp.get_common_dictionary('GLASS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data with H5PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "f = h5py.File(filepath, 'r')\n",
    "print(list(f.keys()))\n",
    "\n",
    "df = ifp.get_common_df(filepath)\n",
    "display(df.describe())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vaex \n",
    "\n",
    "* Using VAEX to visualize big data frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = vaex.open(filepath)\n",
    "# We're parsing the utc_datetime from IceFlow into a data type that vaex understands.\n",
    "df['date'] = df.utc_datetime.values.astype('datetime64[ns]')\n",
    "# my_df = df['longitude', 'latitude', 'elevation', 'date']\n",
    "# Note that we need a common dictionary because in GLAH06 elevation is d_elev and in ICESat-2 is called elevation! \n",
    "my_df = df[df_key['latitude'], df_key['longitude'], df_key['elevation'], 'date']\n",
    "# vaex.vrange() is like numpy.arange but uses 0-memory no matter the length.\n",
    "# This is to down-sample the data for dataviz see: https://github.com/vaexio/vaex/issues/911\n",
    "df.add_column('index', vaex.vrange(0, len(df)))\n",
    "# We are going to create a \"decimated\" dataframe with only 1/100 of the size of the original to plot the big picture faster.\n",
    "df_decimated = df[(df.index % 100 == 0)]\n",
    "my_df.describe()\n",
    "display(my_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Big Picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.widget.heatmap(my_df[df_key['longitude']], \n",
    "               my_df[df_key['latitude']],\n",
    "               what=vaex.stat.mean(my_df[df_key['elevation']]),\n",
    "               shape=512, \n",
    "               figsize=(10,6),\n",
    "               limits='minmax',\n",
    "               colormap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import vaex\n",
    "from ipywidgets import widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "plt.figure(figsize=(10,8), dpi= 90)\n",
    "ax = plt.axes(projection=ccrs.SouthPolarStereo(central_longitude=0)) \n",
    "ax.coastlines(resolution='50m', color='black', linewidth=1)\n",
    "ax.set_extent([-180, 180, -65, -90], ccrs.PlateCarree())\n",
    "plt.scatter(df_decimated[df_key['longitude']].values,\n",
    "            df_decimated[df_key['latitude']].values,\n",
    "            c=df_decimated[df_key['elevation']].values,\n",
    "            cmap='viridis',\n",
    "            vmin=100,vmax=200,\n",
    "            transform=ccrs.PlateCarree())\n",
    "plt.colorbar(label='elevation', shrink=0.5, extend='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Flying\" with the Sensor to Get a Closer Look on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from ipywidgets import widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.view_init(70, 70)\n",
    "\n",
    "#If the data granule(s) is big enough (1+GB), use the decimated dataframe.\n",
    "# df = df_decimated\n",
    "\n",
    "def plot_func(alontrack):\n",
    "    step = 5000 # same as density\n",
    "    m = int(alontrack * step)\n",
    "    ax.clear()\n",
    "    ax.scatter(df[df_key['longitude']].values[m:m+step],\n",
    "               df[df_key['latitude']].values[m:m+step],\n",
    "               df[df_key['elevation']].values[m:m+step],\n",
    "               c=df[df_key['elevation']].values[m:m+step],\n",
    "               cmap='viridis', s=1)\n",
    "    ax.axis('tight')\n",
    "    \n",
    "    \n",
    "\n",
    "interact(plot_func, alontrack = widgets.FloatSlider(value=0,\n",
    "                                                    description='Along Track Steps',\n",
    "                                                    min=0,\n",
    "                                                    max=90,\n",
    "                                                    step=0.3,\n",
    "                                                    layout={'width': '100%'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series\n",
    "\n",
    "After harmonizing data from Pre-IB, IS1, IceBridge, ICESat-2 we should be able to grid and or concatenate the dataframes and start building scientific analyses.\n",
    "\n",
    "some ideas for future IceFlow development: \n",
    "\n",
    "* Use numba when possible (when working with raw numpy arrays, as numba cannot accelerate pandas/dask etc)\n",
    "* Data decimation and gridding, do we really need 1 billion points?\n",
    "* Dask deployed in DAACs not AWS\n",
    "* Extend vaex ala geopandas.\n",
    "* Explore https://github.com/davidbrochart/xarray_leaflet for data exploration.\n",
    "* **Pangeo All the Things!!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
